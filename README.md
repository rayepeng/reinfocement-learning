# 迷宫游戏 Q-学习演示

## 项目概述

这个项目是一个交互式的迷宫游戏，展示了强化学习中的Q-学习算法。通过可视化的方式，用户可以观察智能体如何在迷宫中学习找到最短路径到达目标位置。

## 算法介绍

### Q-学习算法

Q-学习是一种无模型（model-free）的强化学习算法，用于学习在特定环境中的最优行动策略。算法的核心是构建和更新一个Q表（Q-table），该表存储了在每个状态下采取各种行动的预期价值。

#### 核心公式

Q-学习使用以下公式更新Q值：

```
Q(s,a) = Q(s,a) + α * [r + γ * max(Q(s',a')) - Q(s,a)]
```

其中：
- Q(s,a)：在状态s下采取行动a的当前Q值
- α：学习率（learning rate）
- r：即时奖励（reward）
- γ：折扣因子（discount factor）
- max(Q(s',a'))：下一状态s'中所有可能行动的最大Q值
- s'：执行行动a后的新状态

### 探索与利用平衡

算法使用ε-贪心策略（ε-greedy policy）来平衡探索（exploration）和利用（exploitation）：
- 以ε的概率随机选择行动（探索）
- 以1-ε的概率选择当前Q值最高的行动（利用）

## 代码实现分析

### 主要组件

1. **迷宫环境**：
   - 6x6的网格
   - 包含墙壁、起点和终点
   - 可视化显示Q值

2. **Q-学习参数**：
   - 学习率（α）：控制新信息对现有Q值的影响程度
   - 折扣因子（γ）：设置为0.9，平衡即时奖励和未来奖励
   - 探索率（ε）：控制随机探索的概率
   - 模拟速度：控制智能体移动的速度

3. **奖励机制**：
   - 到达目标：+10分
   - 每一步移动：-0.1分（小惩罚以鼓励寻找最短路径）

### 关键函数

- `initializeMaze()`：初始化迷宫环境和Q值
- `getQValue()/setQValue()`：获取和设置Q值
- `chooseAction()`：基于ε-贪心策略选择行动
- `getNextState()`：计算行动后的新状态
- `simulationStep()`：执行Q-学习的一个步骤，包括：
  - 选择行动
  - 获取新状态和奖励
  - 更新Q值
  - 移动智能体

## 用户界面

用户可以通过界面控制：
- 开始/暂停学习过程
- 重置迷宫和Q值
- 调整学习率
- 调整探索率
- 调整模拟速度

界面还显示：
- 当前步数
- 成功到达目标的次数
- 每个单元格的Q值（上、右、下、左四个方向）

## 学习过程可视化

- 智能体初始位于起点
- 根据当前策略移动
- 每个单元格显示四个方向的Q值
- 访问过的单元格会被标记
- 到达目标后，智能体返回起点继续学习

## 如何使用

1. 打开页面后，迷宫会自动初始化
2. 点击"开始学习"按钮启动模拟
3. 使用滑块调整学习参数
4. 观察Q值如何随时间变化
5. 注意智能体逐渐学习到最优路径

## 算法效果分析

随着学习的进行，您将观察到：
1. Q值逐渐收敛
2. 智能体探索的路径越来越有效率
3. 到达目标的步数减少
4. 最优路径的Q值明显高于其他路径

这个演示直观地展示了Q-学习如何通过试错学习找到最优策略，是理解强化学习基本原理的绝佳工具。

## Q值的含义

在代码中，每个单元格显示的四个值（上、右、下、左）代表Q表中的Q值。这些值表示从当前状态采取特定方向行动的预期累积奖励。

初始时，所有Q值都被设置为0，随着学习过程的进行，这些值会根据智能体的经验不断更新。

## 方向编码

在代码中，四个方向的编码如下：
- 0: 上 (Up)
- 1: 右 (Right)
- 2: 下 (Down)
- 3: 左 (Left)

这可以在`qValues[stateId] = [0, 0, 0, 0]`的初始化和后续的方向处理中看到。

## 移动方向的选择逻辑

智能体如何选择移动方向是Q-学习算法的核心。在代码中，这主要由`chooseAction()`函数决定：

```javascript
function chooseAction(row, col) {
    // 探索 vs 利用
    if (Math.random() < explorationRate) {
        // 探索：随机选择一个方向
        return Math.floor(Math.random() * 4);
    } else {
        // 利用：选择Q值最高的方向
        return getBestAction(row, col);
    }
}
```

这里体现了ε-贪心策略：
1. 以`explorationRate`的概率随机选择一个方向（探索）
2. 以`1-explorationRate`的概率选择当前Q值最高的方向（利用）

## 为什么Q值是负数？

您观察到的Q值为负数（如-0.1，-0.2）是因为奖励机制的设计：
- 每一步移动都有-0.1的奖励（小惩罚）
- 只有到达目标时才有+10的奖励

这种设计是为了鼓励智能体寻找最短路径。由于大多数状态都会经历多次-0.1的奖励才能最终获得+10的奖励，所以在学习初期，Q值往往是负的。

## Q值更新过程

Q值的更新是通过以下代码实现的：

```javascript
// 更新Q值
const currentQValue = getQValue(agent.row, agent.col, action);
const maxNextQValue = getMaxQValue(nextState.row, nextState.col);
const newQValue = currentQValue + learningRate * (reward + discountFactor * maxNextQValue - currentQValue);

setQValue(agent.row, agent.col, action, newQValue);
```

这对应Q-学习的核心公式：
```
Q(s,a) = Q(s,a) + α * [r + γ * max(Q(s',a')) - Q(s,a)]
```

## 实际移动方向的决定

随着学习的进行，Q值会逐渐收敛。在利用阶段，智能体会选择Q值最高的方向移动。

例如，如果某个单元格的Q值为：
- 上: -0.5
- 右: -0.2
- 下: -0.8
- 左: -0.3

智能体会选择"右"方向，因为它的Q值最高（-0.2）。

## 算法背后的直觉

Q-学习的核心思想是：
1. 初始时，智能体对环境一无所知，所以需要大量探索
2. 每次行动后，根据获得的奖励和下一状态的最大Q值更新当前状态-行动对的Q值
3. 随着学习的进行，Q值会逐渐反映出从每个状态出发能获得的最大累积奖励
4. 最终，智能体可以通过选择Q值最高的行动来找到最优路径

这就是为什么随着学习的进行，通往目标的最优路径上的单元格会有较高的Q值，而其他路径的Q值会较低。

## 总结

在这个迷宫游戏中，智能体通过Q-学习算法逐渐学习到从起点到终点的最优路径。Q值表示从特定状态采取特定行动的预期累积奖励，智能体会倾向于选择Q值最高的方向移动。负的Q值是由于每步移动的小惩罚，但随着学习的进行，通往目标的路径上的Q值会变得相对更高。
